{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear least squares and parameter estimation\n",
    "\n",
    "Up until now, we have looked at problems where we can fit a straight line or a polynomial to data in a single step, obtaining a unique solution that's guaranted to maximize the likelihood (or equivalently, minimize the misfit between the model and observations. Here, we will look at a couple of examples of non-linear curve fitting or parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will generate some synthetic data. We are going to try to fit a function of the form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic data for fitting\n",
    "x = 5*np.random.rand(100,1)\n",
    "a_true = 1.0\n",
    "b_true = 2.0\n",
    "sigma = 0.03\n",
    "y = a_true*np.exp(-x/b_true) + sigma*np.random.randn(100,1)\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fit a function of the form:\n",
    "$$\n",
    "f(x) = A \\exp(-x/B)\n",
    "$$\n",
    "to the data. In order to do so, we can re-couch the problem as one of residual minimization. We want to minimize the *residual*, defined as the 2-norm of the misfit between the data and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function representing the model\n",
    "def f(parameters,x=x,y=y):\n",
    "    a=parameters[0]\n",
    "    b=parameters[1]\n",
    "    return a*np.exp(-x/b)\n",
    "\n",
    "# define a function that will calculate the value of the residual, \n",
    "#given estimates for the parameters\n",
    "def residual(parameters,x=x,y=y):\n",
    "    delta_y = f(parameters) - y # This is the misfit.\n",
    "    return np.sqrt(np.sum(delta_y**2))\n",
    "\n",
    "def plot_solution(parameters,x=x,y=y):\n",
    "    plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    plt.plot(x,f(parameters))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots illustrating the value of the residual vs. a & b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ntest = 100 # number of values to try along each axis\n",
    "delta = 1.0\n",
    "exact_parameters = [a_true,b_true]\n",
    "fig, (ax1) = plt.subplots(1,1,figsize=(4,4))\n",
    "a = np.linspace(a_true-delta,a_true+delta,ntest)\n",
    "b = np.linspace(b_true-delta,b_true+delta,ntest)\n",
    "\n",
    "r_ab = np.zeros((ntest,ntest))\n",
    "for i in range(ntest):\n",
    "    for j in range(ntest):\n",
    "        parameters = [a[i],b[j]]\n",
    "        r_ab[i,j] = residual(parameters)\n",
    "co=ax1.contourf(a,b,r_ab)\n",
    "plt.colorbar(co,ax=ax1)\n",
    "ax1.set_xlabel('a')\n",
    "ax1.set_ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a 3D plot of the misfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "ax.view_init(30,-120)\n",
    "\n",
    "ls = LightSource(azdeg=0, altdeg=90)\n",
    "cmap = cm.coolwarm\n",
    "rgb = ls.shade(r_ab, cmap)\n",
    "aa,bb = np.meshgrid(a,b)\n",
    "# Plot the surface\n",
    "ax.plot_surface(aa,bb,r_ab,cmap=cmap,rstride=1,cstride=1,\n",
    "                facecolors=rgb,antialiased=False,shade=False)\n",
    "ax.set_xlabel('a')\n",
    "ax.set_ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the grid search from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's try a kind of crummy grid search algorithm (see Bevington chapter 8)\n",
    "# Bevington's algorithm is slightly better than this one in that it usees a parabolic fit to fine-tune the last step.\n",
    "aguess = np.array([5.0,6.0]) # initial guess at the parameters a and b\n",
    "def my_gridsearch(function,aguess,delta=None,maxiter=100):\n",
    "    # the third, optional, argument is a list of increment values by which we perturb the parameters\n",
    "    nparam = len(aguess)\n",
    "    if delta is None:\n",
    "        delta=np.ones(aguess.shape)*.01        \n",
    "    current_a = aguess # current value of the parameters\n",
    "    a_archive = []\n",
    "    a_archive.append(current_a)\n",
    "    iter=1\n",
    "    while iter<maxiter:\n",
    "        f_begin = function(current_a)\n",
    "        for i in range(nparam):\n",
    "            # determine whether to search in +ive or -ve direction:\n",
    "            aplus = current_a.copy() # note - you need to actually make a copy of the array here!!\n",
    "            aplus[i] += delta[i]\n",
    "            aminus = current_a.copy()\n",
    "            aminus[i] -= delta[i]\n",
    "\n",
    "            # evaluate the function at the current location and in the +/- directions\n",
    "            f = function(current_a)\n",
    "            fminus = function(aminus)\n",
    "            fplus = function(aplus)\n",
    "            if( fplus < f):\n",
    "                direction = 1.0\n",
    "            else:\n",
    "                direction = -1.0\n",
    "            while(True):\n",
    "                # continue to search in decreasing direction\n",
    "                f = function(current_a)\n",
    "                a_new = current_a.copy()\n",
    "                a_new[i] += direction*delta[i]\n",
    "                fnew = function(a_new)\n",
    "                if(fnew > f):\n",
    "                    break\n",
    "                elif (fnew < f): # accept the new solution\n",
    "                    current_a = a_new.copy()\n",
    "                    a_archive.append(current_a.copy())\n",
    "                else:\n",
    "                    break\n",
    "            iter +=1\n",
    "        f_end = function(current_a)\n",
    "        if( np.abs(f_end-f_begin)/np.abs(f_begin) < .01 ):\n",
    "            # if improvemnet becomes sufficiently small, stop optimization\n",
    "            print('change is small. stopping')\n",
    "            break\n",
    "    return current_a,a_archive\n",
    "abest,a_archive = my_gridsearch(residual,aguess)\n",
    "print(abest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "a_archive = np.array(a_archive)\n",
    "\n",
    "a = np.linspace(0,8,ntest)\n",
    "b = np.linspace(0,8,ntest)\n",
    "\n",
    "r_ab = np.zeros((ntest,ntest))\n",
    "for i in range(ntest):\n",
    "    for j in range(ntest):\n",
    "        parameters = [a[i],b[j]]\n",
    "        r_ab[i,j] = residual(parameters)\n",
    "\n",
    "\n",
    "fig,ax1 = plt.subplots(1,1)\n",
    "co=ax1.contourf(a,b,r_ab,100)\n",
    "plt.colorbar(co,ax=ax1)\n",
    "ax1.plot(a_archive[:,0],a_archive[:,1],'r.')\n",
    "ax1.set_xlabel('a')\n",
    "ax1.set_ylabel('b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain a least squares solution using SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = [0.5,1.75] #a,b as an initial guess\n",
    "solution = opt.least_squares( residual, initial_guess)\n",
    "print('a=',solution.x[0])\n",
    "print('b=',solution.x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solve it as a global optimization problem using SciPy\n",
    "\n",
    "Another way to treat this problem is to view it as a **global optimization**  problem. That is, we are trying to find the values of parameters $\\underline{p}$ that minimize some (potentially very complicated) function $f(\\underline{p})$. In the case of the problem at hand, $f(\\underline{p})$ is just the misfit (in the 2-norm) between the model and the data. SciPy contains several functions that perform this kind of global optimization using brute force (`scipy.optimize.brute`) or various more sophisticated algorithms. Although the example here has a single local minimum (shown above), many problems of geological interest do not, and we must resort to complicated algorithms to try to find the global minimum of a function of many variables. SciPy has a variety of *black box* optimization routines for finding minimum, and I demonstrate a couple of these below. In general, the global optimization functions will perform best when you can provide reasonable estimatese or constraints on the *bounds* of the parameters. For instance, we can supply *bounds*  on the parameters a and b:\n",
    "\n",
    "$0\\le a \\le 100$\n",
    "\n",
    "$0 \\le b \\le 10$\n",
    "\n",
    "The bounds are passed as arguments to the global optimization functions like so:\n",
    "```bounds = [[a_min,a_max],[b_min,b_max],...]```\n",
    "If we were estimating N parameters, we would supply an Nx2 list of bounds on each of the N parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use  the 'dual annealing' algorithm\n",
    "bounds = [[.01,100],[.01,10]]\n",
    "opt.dual_annealing(residual,bounds) # we pass the name of the function that we are mininizing and the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use brute force (not recommended for more than a couple of parameters)\n",
    "opt.brute(residual,bounds,Ns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evil functions with multiple local minima\n",
    "\n",
    "Now, let's try to find the minima of a more complicated function that posesses local minima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params):\n",
    "    x=params[0]\n",
    "    y=params[1]\n",
    "    return 0.1*np.sqrt(x**2 + y**2) - np.cos(np.pi*x/5)*np.cos(np.pi*y/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-10,10,ntest)\n",
    "yy = np.linspace(-10,10,ntest)\n",
    "fvals = np.zeros((ntest,ntest))\n",
    "for i in range(ntest):\n",
    "    for j in range(ntest):\n",
    "        p=[xx[i],yy[j]]\n",
    "        fvals[j,i] = f(p)\n",
    "plt.figure()\n",
    "plt.contourf(xx,yy,fvals,100)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global minimum of $f$ occurs at the origin, but there are many other local minima, as seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [[-10,10],[-10,10]]\n",
    "opt.brute(f,bounds,Ns=20,finish=opt.fmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - this gets stuck in a local minimum!!\n",
    "opt.dual_annealing(residual,bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try least squares with a bad initial guess\n",
    "initial_guess = [5,5]\n",
    "solution = opt.least_squares( residual, initial_guess)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = opt.basinhopping(residual,initial_guess,niter=1000,T=1.0,stepsize=4.01)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
